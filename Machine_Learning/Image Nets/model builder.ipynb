{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1\n",
      "INFO:root:2\n",
      "WARNING:root:3\n",
      "CRITICAL:root:4\n",
      "ERROR:root:5\n"
     ]
    }
   ],
   "source": [
    "logging.debug(1)\n",
    "logging.info(2)\n",
    "logging.warning(3)\n",
    "logging.critical(4)\n",
    "logging.error(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "TRAIN_DIR = \"../Cat_Dog/train\"\n",
    "\n",
    "def convert_to_one_hot_label(img_name):\n",
    "    label = img_name.split(\".\")[0]\n",
    "    return [1, 0] if label == \"cat\" else [0, 1]\n",
    "\n",
    "def load_train_data(resize_pics=(227, 227)):\n",
    "    width = resize_pics[0]\n",
    "    height = resize_pics[1]\n",
    "    FILE_NAME = \"train_data_{}x{}.npy\".format(width, height)\n",
    "    if os.path.exists(FILE_NAME):\n",
    "        logging.info(\"load train data\")\n",
    "        train_data = np.load(FILE_NAME)\n",
    "    else:\n",
    "        logging.info(\"create train data\")\n",
    "        train_data = []\n",
    "        for img_name in tqdm(os.listdir(TRAIN_DIR)):\n",
    "            one_hot_label = convert_to_one_hot_label(img_name)\n",
    "            path = os.path.join(TRAIN_DIR, img_name)\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, (width, height))\n",
    "            train_data.append([img, np.array(one_hot_label)])\n",
    "        logging.info(\"finish load to memory\")\n",
    "        np.random.shuffle(train_data)\n",
    "        logging.info(\"finish shuffle\")\n",
    "        np.save(FILE_NAME, train_data)\n",
    "        logging.info(\"finish save to file\")\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:load train data\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data(resize_pics=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:(25000, 128, 128, 3), (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_data[:, 0], train_data[:, 1]\n",
    "X = np.array([i for i in X])\n",
    "Y = np.array([i for i in Y])\n",
    "logging.debug(\"{}, {}\".format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"AlexNet_basic_2\"\n",
    "NUM_CLASS = 2  # Cat, Dog\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def alexnet(num_class):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, 128, 128, 3])  # 원래 227x227\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, num_class, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    \n",
    "    return network\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(alexnet(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=5, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9152  | total loss: 0.12015 | time: 34.573s\n",
      "| Momentum | epoch: 026 | loss: 0.12015 - acc: 0.9585 -- iter: 22500/22500\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=6, validation_set=0.1, shuffle=True,\n",
    "              show_metric=True, batch_size=64, snapshot_step=200,\n",
    "              snapshot_epoch=False, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/AlexNet_basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:create train data\n",
      "100%|████████████████████████████████| 25000/25000 [01:08<00:00, 367.20it/s]\n",
      "INFO:root:finish load to memory\n",
      "INFO:root:finish shuffle\n",
      "INFO:root:finish save to file\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data(resize_pics=(32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 64, 64, 3), (25000, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = train_data[:, 0], train_data[:, 1]\n",
    "X = np.array([i for i in X])\n",
    "Y = np.array([i for i in Y])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"VGG16_basic_32x32_try\"\n",
    "NUM_CLASS = 2\n",
    "tf.reset_default_graph()\n",
    "def vgg16(num_class):\n",
    "    # Building 'VGG Network'\n",
    "    x = tflearn.input_data(shape=[None, 32, 32, 3], name='input')  # 원래는 224x224\n",
    "    x = tflearn.conv_2d(x, 64, 3, activation='relu', scope='conv1_1')\n",
    "    x = tflearn.conv_2d(x, 64, 3, activation='relu', scope='conv1_2')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool1')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 128, 3, activation='relu', scope='conv2_1')\n",
    "    x = tflearn.conv_2d(x, 128, 3, activation='relu', scope='conv2_2')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool2')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_1')\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_2')\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_3')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool3')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_1')\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_2')\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_3')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool4')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_1')\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_2')\n",
    "    x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_3')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool5')\n",
    "\n",
    "    x = tflearn.fully_connected(x, 4096, activation='relu', scope='fc6')\n",
    "    x = tflearn.dropout(x, 0.5, name='dropout1')\n",
    "\n",
    "    x = tflearn.fully_connected(x, 4096, activation='relu', scope='fc7')\n",
    "    x = tflearn.dropout(x, 0.5, name='dropout2')\n",
    "\n",
    "    x = tflearn.fully_connected(x, num_class, activation='softmax', scope='fc8')\n",
    "    x = regression(x, optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   learning_rate=0.003)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(vgg16(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, n_epoch=10, validation_set=0.1, shuffle=True,\n",
    "        show_metric=True, batch_size=64, snapshot_epoch=False,\n",
    "        snapshot_step=500, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"VGG16_basic_32x32_light\"\n",
    "NUM_CLASS = 2\n",
    "tf.reset_default_graph()\n",
    "def vgg16(num_class):\n",
    "    # Building 'VGG Network'\n",
    "    x = tflearn.input_data(shape=[None, 32, 32, 3], name='input')  # 원래는 224x224\n",
    "    x = tflearn.conv_2d(x, 64, 3, activation='relu', scope='conv1_1')\n",
    "    x = tflearn.conv_2d(x, 64, 3, activation='relu', scope='conv1_2')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool1')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 128, 3, activation='relu', scope='conv2_1')\n",
    "    x = tflearn.conv_2d(x, 128, 3, activation='relu', scope='conv2_2')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool2')\n",
    "\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_1')\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_2')\n",
    "    x = tflearn.conv_2d(x, 256, 3, activation='relu', scope='conv3_3')\n",
    "    x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool3')\n",
    "\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_1')\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_2')\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv4_3')\n",
    "#     x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool4')\n",
    "\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_1')\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_2')\n",
    "#     x = tflearn.conv_2d(x, 512, 3, activation='relu', scope='conv5_3')\n",
    "#     x = tflearn.max_pool_2d(x, 2, strides=2, name='maxpool5')\n",
    "\n",
    "    x = tflearn.fully_connected(x, 512, activation='relu', scope='fc6')\n",
    "    x = tflearn.dropout(x, 0.5, name='dropout1')\n",
    "\n",
    "    x = tflearn.fully_connected(x, 512, activation='relu', scope='fc7')\n",
    "    x = tflearn.dropout(x, 0.5, name='dropout2')\n",
    "\n",
    "    x = tflearn.fully_connected(x, num_class, activation='softmax', scope='fc8')\n",
    "    x = regression(x, optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   learning_rate=0.003)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(vgg16(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, n_epoch=10, validation_set=0.1, shuffle=True,\n",
    "        show_metric=True, batch_size=64, snapshot_epoch=False,\n",
    "        snapshot_step=500, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:load train data\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data(resize_pics=(32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 32, 32, 3), (25000, 2))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = train_data[:, 0], train_data[:, 1]\n",
    "X = np.array([i for i in X])\n",
    "Y = np.array([i for i in Y])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ResNet_basic\"\n",
    "NUM_CLASS = 2\n",
    "# Residual blocks\n",
    "# 32 layers: n=5, 56 layers: n=9, 110 layers: n=18\n",
    "BLOCK = 5\n",
    "tf.reset_default_graph()\n",
    "def resnet(num_class):\n",
    "    # Building 'Residual Network'\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "#     img_aug = tflearn.ImageAugmentation()\n",
    "#     img_aug.add_random_flip_leftright()\n",
    "#     img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "    # Building Residual Network\n",
    "    net = tflearn.input_data(shape=[None, 32, 32, 3])#,data_augmentation=img_aug)\n",
    "    net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.residual_block(net, BLOCK, 16)\n",
    "    net = tflearn.residual_block(net, 1, 32, downsample=True)\n",
    "    net = tflearn.residual_block(net, BLOCK-1, 32)\n",
    "    net = tflearn.residual_block(net, 1, 64, downsample=True)\n",
    "    net = tflearn.residual_block(net, BLOCK-1, 64)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    \n",
    "    # Regression\n",
    "    net = tflearn.fully_connected(net, num_class, activation='softmax')\n",
    "    mom = tflearn.Momentum(0.1, lr_decay=0.1, decay_step=32000, staircase=True)\n",
    "    net = tflearn.regression(net, optimizer=mom,\n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    return net\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(resnet(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WinterJ\\Documents\\GitHub\\python\\Code_Study\\Machine_Learning\\Image Nets\\model\\ResNet_basic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WinterJ\\Documents\\GitHub\\python\\Code_Study\\Machine_Learning\\Image Nets\\model\\ResNet_basic\n"
     ]
    }
   ],
   "source": [
    "model.load(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9152  | total loss: 0.11216 | time: 33.324s\n",
      "| Momentum | epoch: 006 | loss: 0.11216 - acc: 0.9667 -- iter: 22500/22500\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=5, validation_set=0.1,\n",
    "    snapshot_epoch=False, snapshot_step=500,\n",
    "    show_metric=True, batch_size=64, shuffle=True,\n",
    "    run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 56, 110 layers ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ResNet_56\"\n",
    "NUM_CLASS = 2\n",
    "# Residual blocks\n",
    "# 32 layers: n=5, 56 layers: n=9, 110 layers: n=18\n",
    "BLOCK = 9\n",
    "tf.reset_default_graph()\n",
    "def resnet_56(num_class):\n",
    "    # Building 'Residual Network'\n",
    "    \n",
    "    # Real-time data augmentation\n",
    "#     img_aug = tflearn.ImageAugmentation()\n",
    "#     img_aug.add_random_flip_leftright()\n",
    "#     img_aug.add_random_crop([32, 32], padding=4)\n",
    "\n",
    "    # Building Residual Network\n",
    "    net = tflearn.input_data(shape=[None, 32, 32, 3])#,data_augmentation=img_aug)\n",
    "    net = tflearn.conv_2d(net, 16, 3, regularizer='L2', weight_decay=0.0001)\n",
    "    net = tflearn.residual_block(net, BLOCK, 16)\n",
    "    net = tflearn.residual_block(net, 1, 32, downsample=True)\n",
    "    net = tflearn.residual_block(net, BLOCK-1, 32)\n",
    "    net = tflearn.residual_block(net, 1, 64, downsample=True)\n",
    "    net = tflearn.residual_block(net, BLOCK-1, 64)\n",
    "    net = tflearn.batch_normalization(net)\n",
    "    net = tflearn.activation(net, 'relu')\n",
    "    net = tflearn.global_avg_pool(net)\n",
    "    \n",
    "    # Regression\n",
    "    net = tflearn.fully_connected(net, num_class, activation='softmax')\n",
    "    mom = tflearn.Momentum(0.1, lr_decay=0.1, decay_step=32000, staircase=True)\n",
    "    net = tflearn.regression(net, optimizer=mom,\n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    return net\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(resnet_56(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9152  | total loss: 0.10521 | time: 81.504s\n",
      "| Momentum | epoch: 026 | loss: 0.10521 - acc: 0.9680 -- iter: 22500/22500\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=1, validation_set=0.1,\n",
    "    snapshot_epoch=False, snapshot_step=500,\n",
    "    show_metric=True, batch_size=64, shuffle=True,\n",
    "    run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:load train data\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data(resize_pics=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 128, 128, 3), (25000, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = train_data[:, 0], train_data[:, 1]\n",
    "X = np.array([i for i in X])\n",
    "Y = np.array([i for i in Y])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.conv import avg_pool_2d\n",
    "\n",
    "MODEL_NAME = \"GoogleNet_basic\"\n",
    "NUM_CLASS = 2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def googlenet(num_class):\n",
    "    network = input_data(shape=[None, 128, 128, 3])  # 원래는 227x227\n",
    "    conv1_7_7 = conv_2d(network, 64, 7, strides=2, activation='relu', name = 'conv1_7_7_s2')\n",
    "    pool1_3_3 = max_pool_2d(conv1_7_7, 3,strides=2)\n",
    "    pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "    conv2_3_3_reduce = conv_2d(pool1_3_3, 64,1, activation='relu',name = 'conv2_3_3_reduce')\n",
    "    conv2_3_3 = conv_2d(conv2_3_3_reduce, 192,3, activation='relu', name='conv2_3_3')\n",
    "    conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "    pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "    inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "    inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96,1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "    inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128,filter_size=3,  activation='relu', name = 'inception_3a_3_3')\n",
    "    inception_3a_5_5_reduce = conv_2d(pool2_3_3,16, filter_size=1,activation='relu', name ='inception_3a_5_5_reduce' )\n",
    "    inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name= 'inception_3a_5_5')\n",
    "    inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, )\n",
    "    inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "\n",
    "    # merge the inception_3a__\n",
    "    inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "    inception_3b_1_1 = conv_2d(inception_3a_output, 128,filter_size=1,activation='relu', name= 'inception_3b_1_1' )\n",
    "    inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "    inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3,  activation='relu',name='inception_3b_3_3')\n",
    "    inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name = 'inception_3b_5_5_reduce')\n",
    "    inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name = 'inception_3b_5_5')\n",
    "    inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "    inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1,activation='relu', name='inception_3b_pool_1_1')\n",
    "\n",
    "    #merge the inception_3b_*\n",
    "    inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat',axis=3,name='inception_3b_output')\n",
    "\n",
    "    pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "    inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "    inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "    inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "    inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "    inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "    inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "\n",
    "    inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "\n",
    "    inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "    inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "    inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "    inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "    inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "\n",
    "    inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "    inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "\n",
    "    inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "\n",
    "    inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu',name='inception_4c_1_1')\n",
    "    inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "    inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "    inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "    inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "\n",
    "    inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "    inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "\n",
    "    inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3,name='inception_4c_output')\n",
    "\n",
    "    inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "    inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "    inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "    inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "    inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "    inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "    inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "\n",
    "    inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "    inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "    inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "    inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "    inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "    inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "    inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "    inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "\n",
    "\n",
    "    inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5,inception_4e_pool_1_1],axis=3, mode='concat')\n",
    "\n",
    "    pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "\n",
    "    inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "    inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "    inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "    inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "    inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "    inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "    inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1,activation='relu', name='inception_5a_pool_1_1')\n",
    "\n",
    "    inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3,mode='concat')\n",
    "\n",
    "\n",
    "    inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1,activation='relu', name='inception_5b_1_1')\n",
    "    inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "    inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3,activation='relu', name='inception_5b_3_3')\n",
    "    inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "    inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce,128, filter_size=5,  activation='relu', name='inception_5b_5_5' )\n",
    "    inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "    inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "    inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "    pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "    pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "    loss = fully_connected(pool5_7_7, num_class,activation='softmax')\n",
    "    network = regression(loss, optimizer='momentum',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "    \n",
    "    return network\n",
    "\n",
    "model = tflearn.DNN(googlenet(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WinterJ\\Documents\\GitHub\\python\\Code_Study\\Machine_Learning\\Image Nets\\model\\GoogleNet_basic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WinterJ\\Documents\\GitHub\\python\\Code_Study\\Machine_Learning\\Image Nets\\model\\GoogleNet_basic\n"
     ]
    }
   ],
   "source": [
    "model.load(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9152  | total loss: 0.10724 | time: 162.380s\n",
      "| Momentum | epoch: 006 | loss: 0.10724 - acc: 0.9598 -- iter: 22500/22500\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=5, validation_set=0.1, shuffle=True,\n",
    "          show_metric=True, batch_size=64, snapshot_step=500,\n",
    "          snapshot_epoch=False, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:load train data\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data(resize_pics=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 128, 128, 3), (25000, 2))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = train_data[:, 0], train_data[:, 1]\n",
    "X = np.array([i for i in X])\n",
    "Y = np.array([i for i in Y])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn.activations as activations\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import flatten\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.utils import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block35(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 32, 3, bias=False, activation=None,name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(net, 32, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 48,3, bias=False, activation=None, name='Conv2d_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 64,3, bias=False, activation=None, name='Conv2d_0c_3x3')))\n",
    "    tower_mixed = merge([tower_conv, tower_conv1_1, tower_conv2_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "def block17(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv_1_0 = relu(batch_normalization(conv_2d(net, 128, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv_1_1 = relu(batch_normalization(conv_2d(tower_conv_1_0, 160,[1,7], bias=False, activation=None,name='Conv2d_0b_1x7')))\n",
    "    tower_conv_1_2 = relu(batch_normalization(conv_2d(tower_conv_1_1, 192, [7,1], bias=False, activation=None,name='Conv2d_0c_7x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv_1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def block8(net, scale=1.0, activation=\"relu\"):\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 192, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 224, [1,3], bias=False, activation=None, name='Conv2d_0b_1x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 256, [3,1], bias=False, name='Conv2d_0c_3x1')))\n",
    "    tower_mixed = merge([tower_conv,tower_conv1_2], mode='concat', axis=3)\n",
    "    tower_out = relu(batch_normalization(conv_2d(tower_mixed, net.get_shape()[3], 1, bias=False, activation=None, name='Conv2d_1x1')))\n",
    "    net += scale * tower_out\n",
    "    if activation:\n",
    "        if isinstance(activation, str):\n",
    "            net = activations.get(activation)(net)\n",
    "        elif hasattr(activation, '__call__'):\n",
    "            net = activation(net)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation.\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, Y = oxflower17.load_data(one_hot=True, resize_pics=(299, 299))\n",
    "MODEL_NAME = \"Inception_ResNet_basic\"\n",
    "NUM_CLASS = 2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def inception_resnet(num_class):\n",
    "    num_classes = num_class\n",
    "    dropout_keep_prob = 0.8\n",
    "\n",
    "    network = input_data(shape=[None, 128, 128, 3])  # 원래 299x299\n",
    "    conv1a_3_3 = relu(batch_normalization(conv_2d(network, 32, 3, strides=2, bias=False, padding='VALID',activation=None,name='Conv2d_1a_3x3')))\n",
    "    conv2a_3_3 = relu(batch_normalization(conv_2d(conv1a_3_3, 32, 3, bias=False, padding='VALID',activation=None, name='Conv2d_2a_3x3')))\n",
    "    conv2b_3_3 = relu(batch_normalization(conv_2d(conv2a_3_3, 64, 3, bias=False, activation=None, name='Conv2d_2b_3x3')))\n",
    "    maxpool3a_3_3 = max_pool_2d(conv2b_3_3, 3, strides=2, padding='VALID', name='MaxPool_3a_3x3')\n",
    "    conv3b_1_1 = relu(batch_normalization(conv_2d(maxpool3a_3_3, 80, 1, bias=False, padding='VALID',activation=None, name='Conv2d_3b_1x1')))\n",
    "    conv4a_3_3 = relu(batch_normalization(conv_2d(conv3b_1_1, 192, 3, bias=False, padding='VALID',activation=None, name='Conv2d_4a_3x3')))\n",
    "    maxpool5a_3_3 = max_pool_2d(conv4a_3_3, 3, strides=2, padding='VALID', name='MaxPool_5a_3x3')\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(maxpool5a_3_3, 96, 1, bias=False, activation=None, name='Conv2d_5b_b0_1x1')))\n",
    "\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 48, 1, bias=False, activation=None, name='Conv2d_5b_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 64, 5, bias=False, activation=None, name='Conv2d_5b_b1_0b_5x5')))\n",
    "\n",
    "    tower_conv2_0 = relu(batch_normalization(conv_2d(maxpool5a_3_3, 64, 1, bias=False, activation=None, name='Conv2d_5b_b2_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2_0, 96, 3, bias=False, activation=None, name='Conv2d_5b_b2_0b_3x3')))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 96, 3, bias=False, activation=None,name='Conv2d_5b_b2_0c_3x3')))\n",
    "\n",
    "    tower_pool3_0 = avg_pool_2d(maxpool5a_3_3, 3, strides=1, padding='same', name='AvgPool_5b_b3_0a_3x3')\n",
    "    tower_conv3_1 = relu(batch_normalization(conv_2d(tower_pool3_0, 64, 1, bias=False, activation=None,name='Conv2d_5b_b3_0b_1x1')))\n",
    "\n",
    "    tower_5b_out = merge([tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(tower_5b_out, 10, block35, scale=0.17)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 384, 3, bias=False, strides=2,activation=None, padding='VALID', name='Conv2d_6a_b0_0a_3x3')))\n",
    "    tower_conv1_0 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_6a_b1_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1_0, 256, 3, bias=False, activation=None, name='Conv2d_6a_b1_0b_3x3')))\n",
    "    tower_conv1_2 = relu(batch_normalization(conv_2d(tower_conv1_1, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_6a_b1_0c_3x3')))\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID',name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv, tower_conv1_2, tower_pool], mode='concat', axis=3)\n",
    "    net = repeat(net, 20, block17, scale=0.1)\n",
    "\n",
    "    tower_conv = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, activation=None, name='Conv2d_0a_1x1')))\n",
    "    tower_conv0_1 = relu(batch_normalization(conv_2d(tower_conv, 384, 3, bias=False, strides=2, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "\n",
    "    tower_conv1 = relu(batch_normalization(conv_2d(net, 256, 1, bias=False, padding='VALID', activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv1_1 = relu(batch_normalization(conv_2d(tower_conv1,288,3, bias=False, strides=2, padding='VALID',activation=None, name='COnv2d_1a_3x3')))\n",
    "\n",
    "    tower_conv2 = relu(batch_normalization(conv_2d(net, 256,1, bias=False, activation=None,name='Conv2d_0a_1x1')))\n",
    "    tower_conv2_1 = relu(batch_normalization(conv_2d(tower_conv2, 288,3, bias=False, name='Conv2d_0b_3x3',activation=None)))\n",
    "    tower_conv2_2 = relu(batch_normalization(conv_2d(tower_conv2_1, 320, 3, bias=False, strides=2, padding='VALID',activation=None, name='Conv2d_1a_3x3')))\n",
    "\n",
    "    tower_pool = max_pool_2d(net, 3, strides=2, padding='VALID', name='MaxPool_1a_3x3')\n",
    "    net = merge([tower_conv0_1, tower_conv1_1,tower_conv2_2, tower_pool], mode='concat', axis=3)\n",
    "\n",
    "    net = repeat(net, 9, block8, scale=0.2)\n",
    "    net = block8(net, activation=None)\n",
    "\n",
    "    net = relu(batch_normalization(conv_2d(net, 1536, 1, bias=False, activation=None, name='Conv2d_7b_1x1')))\n",
    "    net = avg_pool_2d(net, net.get_shape().as_list()[1:3],strides=2, padding='VALID', name='AvgPool_1a_8x8')\n",
    "    net = flatten(net)\n",
    "    net = dropout(net, dropout_keep_prob)\n",
    "    loss = fully_connected(net, num_classes,activation='softmax')\n",
    "\n",
    "\n",
    "    network = tflearn.regression(loss, optimizer='RMSprop',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.0001)\n",
    "    \n",
    "    return network\n",
    "\n",
    "model = tflearn.DNN(inception_resnet(NUM_CLASS), checkpoint_path='checkpoint/model_{}'.format(MODEL_NAME),\n",
    "                    max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, n_epoch=7, validation_set=0.1, shuffle=True,\n",
    "          show_metric=True, batch_size=16, snapshot_step=500,\n",
    "          snapshot_epoch=False, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model/\" + MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
